{"cells":[{"cell_type":"markdown","metadata":{"id":"_47YZDI-Eow3"},"source":["# Supervised learning: diverse classifiers\n","\n","*   Section 1. Baseline logistic regression.\n","*   Section 2. ML classifiers.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t4ptBsepkoZS"},"source":["The scripts include more than the seven classifiers we discussed in the class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p5QS9WvXEPh6"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NqP5RqzjFYtM"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, log_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5tF1hWGHPkO"},"outputs":[],"source":["## import the sklearn models\n","# logistic regression\n","from sklearn.linear_model import LogisticRegression\n","\n","# K nearest neighbor\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# support vector machine\n","from sklearn.svm import SVC, LinearSVC\n","# SVC: support vector classification (using kernel methods)\n","\n","# decision tree\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# ensemble methods, e.g., random forest\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","\n","# naive Bayesian\n","from sklearn.naive_bayes import GaussianNB\n","\n","# discriminant analysis\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","\n","# neural network\n","from sklearn.neural_network import MLPClassifier\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17327,"status":"ok","timestamp":1680635471810,"user":{"displayName":"Shenhao Wang","userId":"03503527554188089407"},"user_tz":240},"id":"Z0TGxYhvEv5g","outputId":"0f7bc3e2-6e39-4cb1-a647-16ff29850a1a"},"outputs":[],"source":["# define the mounting point on Google drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","# Switch to Colab Notebooks.\n","# Mac system\n","# !cd '/content/drive/My Drive/Colab Notebooks/data/'\n","# Windows system\n","%cd /content/drive/My Drive/Colab Notebooks/data/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W53AKJX0E1U5"},"outputs":[],"source":["# read data\n","df = pd.read_csv('Florida_ct.csv', index_col = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1680635472625,"user":{"displayName":"Shenhao Wang","userId":"03503527554188089407"},"user_tz":240},"id":"v_PjycOBE89o","outputId":"958ec0dd-c930-40e6-a06e-a4ec2f512481"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3cU_RJ5E9Fc"},"outputs":[],"source":["# preprocessing\n","# expensive vs. non-expensive properties as the binary variable\n","# threshold = 0.8\n","df['property_value_discrete'] = 1\n","df.loc[df['property_value_median'] < 200000, 'property_value_discrete'] = 0"]},{"cell_type":"markdown","metadata":{"id":"oUbGlX53FLGg"},"source":["## Section 1. Creating a baseline logistic regression in ML (lec08)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CAyM5ZnFKkA"},"outputs":[],"source":["# assign the inputs and outputs\n","var_list = ['inc_median_household',\n","            'households',\n","            'travel_driving_ratio', 'travel_pt_ratio', 'travel_taxi_ratio', 'travel_work_home_ratio',\n","            'edu_higher_edu_ratio',\n","            'household_size_avg',\n","            'vacancy_ratio', 'rent_median',\n","            'race_white_ratio',\n","            'race_asian_ratio'\n","            ]\n","\n","y = df['property_value_discrete']\n","X = df[var_list]\n","# X = sm.add_constant(X)\n","\n","# change the data format\n","X = X.values\n","y = y.values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wELrRzp0E9Jl"},"outputs":[],"source":["# creating the training and testing split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":199,"status":"ok","timestamp":1680635575492,"user":{"displayName":"Shenhao Wang","userId":"03503527554188089407"},"user_tz":240},"id":"5rbRQyoOE9L8","outputId":"c08cb748-c8f5-4c28-f301-20ceea86f0d1"},"outputs":[],"source":["# initialize logistic regression\n","# instantiate the model (using the default parameters)\n","logreg = LogisticRegression(random_state=16)\n","\n","# fit the model with training data only\n","logreg.fit(X_train, y_train)\n","\n","# check the performance\n","train_predictions = logreg.predict(X_train)\n","acc = accuracy_score(y_train, train_predictions)\n","print(\"Training Accuracy: {:.4%}\".format(acc))\n","\n","test_predictions = logreg.predict(X_test)\n","acc = accuracy_score(y_test, test_predictions)\n","print(\"Testing Accuracy: {:.4%}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"uzbRvKVtHhs5"},"source":["## Section 2. ML classifiers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14652,"status":"ok","timestamp":1680635919388,"user":{"displayName":"Shenhao Wang","userId":"03503527554188089407"},"user_tz":240},"id":"qnwYeeSDFor_","outputId":"2472b652-0a71-40d0-eb76-bebabbb67030"},"outputs":[],"source":["# check the performance for all the classifiers.\n","classifiers = [\n","    LogisticRegression(random_state=16),\n","    KNeighborsClassifier(3),\n","    SVC(kernel=\"rbf\", C=0.025, probability=True),\n","    DecisionTreeClassifier(),\n","    RandomForestClassifier(),\n","    GaussianNB(),\n","    MLPClassifier(alpha=1e-10, hidden_layer_sizes=(20, 2), random_state=1),\n","    AdaBoostClassifier(),\n","    GradientBoostingClassifier(),\n","    LinearDiscriminantAnalysis(),\n","    QuadraticDiscriminantAnalysis()]\n","\n","# Logging for Visual Comparison\n","log_cols=[\"Classifier\", \"Train Accuracy\", \"Train Log Loss\", \"Test Accuracy\", \"Test Log Loss\"]\n","log = pd.DataFrame(columns=log_cols)\n","\n","for clf in classifiers:\n","    clf.fit(X_train, y_train)\n","    name = clf.__class__.__name__\n","\n","    print(\"=\"*30)\n","    print(name)\n","\n","    print('****Results****')\n","    # training\n","    train_predictions = clf.predict(X_train)\n","    train_acc = accuracy_score(y_train, train_predictions)\n","    print(\"Training Accuracy: {:.4%}\".format(train_acc))\n","\n","    train_predictions = clf.predict_proba(X_train)\n","    train_ll = log_loss(y_train, train_predictions)\n","    print(\"Training Log Loss: {}\".format(train_ll))\n","\n","    # testing\n","    test_predictions = clf.predict(X_test)\n","    test_acc = accuracy_score(y_test, test_predictions)\n","    print(\"Testing Accuracy: {:.4%}\".format(test_acc))\n","\n","    test_predictions = clf.predict_proba(X_test)\n","    test_ll = log_loss(y_test, test_predictions)\n","    print(\"Testing Log Loss: {}\".format(test_ll))\n","\n","    log_entry = pd.DataFrame([[name, train_acc*100, train_ll, test_acc*100, test_ll]], columns=log_cols)\n","    log = pd.concat([log, log_entry])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":517,"status":"ok","timestamp":1680636260456,"user":{"displayName":"Shenhao Wang","userId":"03503527554188089407"},"user_tz":240},"id":"82MPtqx5FwtL","outputId":"33a3a2a6-0eab-474c-9128-87ff66f4e40f"},"outputs":[],"source":["# Visualize the performance.\n","sns.set_color_codes(\"muted\")\n","sns.barplot(x='Test Accuracy', y='Classifier', data=log, color=\"b\")\n","plt.xlabel(' Accuracy %')\n","plt.title('Classifier Accuracy')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":586,"status":"ok","timestamp":1680636341405,"user":{"displayName":"Shenhao Wang","userId":"03503527554188089407"},"user_tz":240},"id":"hx3I3G0fFwrF","outputId":"2068cb2b-da84-4161-92ae-dba8dabac7d0"},"outputs":[],"source":["# Visualize the performance.\n","sns.set_color_codes(\"muted\")\n","sns.barplot(x='Test Log Loss', y='Classifier', data=log, color=\"g\")\n","plt.xlabel('Log Loss')\n","plt.title('Classifier Log Loss')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"vkti_RoEns0X"},"source":["\n","\n","\n","*   The performance of the logistic regression is not bad, although usually it is not the highest.\n","*   The ensemble methods, e.g. RF, achieve the highest predictive performance.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pVMr-CgyoGp4"},"source":["## **Exercise.** Create a dummy variable to represent the auto vs. non-auto census tracts by using a threshold value. Then compare the performance of the ML classifiers.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qq51QBXCFwYE"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNiceeuKOjBAq8Keea9fttU","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
